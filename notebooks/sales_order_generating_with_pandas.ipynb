{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to customers2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "\n",
    "def create_customers(num_customers=10000, filename='customers.csv'):\n",
    "    fake = Faker()\n",
    "    customer_data = {\n",
    "        'CustomerID': np.arange(1, num_customers + 1),\n",
    "        'FirstName': [fake.first_name() for _ in range(num_customers)],\n",
    "        'LastName': [fake.last_name() for _ in range(num_customers)],\n",
    "        'Email': [fake.email() for _ in range(num_customers)],\n",
    "        'Address': [fake.address().replace('\\n', ', ') for _ in range(num_customers)]\n",
    "    }\n",
    "    df = pd.DataFrame(customer_data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f'Data saved to {filename}')\n",
    "\n",
    "# Call the function\n",
    "create_customers(num_customers=100000, filename='customers2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to customers3.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "def generate_customer_data(num_customers):\n",
    "    fake = Faker()\n",
    "    customer_data = {\n",
    "        'CustomerID': np.arange(1, num_customers + 1),\n",
    "        'FirstName': [fake.first_name() for _ in range(num_customers)],\n",
    "        'LastName': [fake.last_name() for _ in range(num_customers)],\n",
    "        'Email': [fake.email() for _ in range(num_customers)],\n",
    "        'Address': [fake.address().replace('\\n', ', ') for _ in range(num_customers)]\n",
    "    }\n",
    "    return pd.DataFrame(customer_data)\n",
    "\n",
    "def create_customers_multiprocessing(num_customers=100_000, filename='customers3.csv'):\n",
    "    num_processes = cpu_count()\n",
    "    pool = Pool(processes=num_processes)\n",
    "\n",
    "    # Divide the work among processes\n",
    "    num_customers_per_process = num_customers // num_processes\n",
    "    args = [num_customers_per_process] * num_processes\n",
    "\n",
    "    # Generate data in parallel\n",
    "    dataframes = pool.map(generate_customer_data, args)\n",
    "\n",
    "    # Combine the dataframes\n",
    "    df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Save to csv\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f'Data saved to {filename}')\n",
    "\n",
    "# Call the function\n",
    "create_customers_multiprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "from multiprocessing import Process, Value, Lock\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "def generate_sales_data(\n",
    "    n_orders_per_process, \n",
    "    order_date, \n",
    "    max_customer_id_shared,\n",
    "    start_sales_order_id,\n",
    "    lock,\n",
    "    process_id\n",
    "):\n",
    "    customers_df = pd.DataFrame(columns=[\"CustomerID\", \"Name\", \"Email\", \"Age\", \"ModifiedDate\"])\n",
    "    sales_order_header_df = pd.DataFrame(columns=[\"SalesOrderID\", \"OrderDate\", \"CustomerID\"])\n",
    "    sales_order_detail_df = pd.DataFrame(columns=[\"SalesOrderID\", \"SalesOrderLineNumber\", \"ProductKey\", \"Qty\", \"ModifiedDate\"])\n",
    "\n",
    "    for i in range(n_orders_per_process):\n",
    "        if randint(1, 10) > 8:\n",
    "            lock.acquire()\n",
    "            max_customer_id_shared.value += 1\n",
    "            curr_customer_id = max_customer_id_shared.value\n",
    "            lock.release()\n",
    "            \n",
    "            new_customer = {\n",
    "                \"CustomerID\": curr_customer_id,\n",
    "                \"Name\": fake.name(),\n",
    "                \"Email\": fake.email(),\n",
    "                \"Age\": randint(18, 90),\n",
    "                \"ModifiedDate\": datetime.now()\n",
    "            }\n",
    "            customers_df = customers_df.append(new_customer, ignore_index=True)\n",
    "        else:\n",
    "            lock.acquire()\n",
    "            curr_customer_id = randint(1, max_customer_id_shared.value)\n",
    "            lock.release()\n",
    "\n",
    "        new_sales_order_header = {\n",
    "            \"SalesOrderID\": start_sales_order_id + i,\n",
    "            \"OrderDate\": order_date,\n",
    "            \"CustomerID\": curr_customer_id\n",
    "        }\n",
    "        sales_order_header_df = sales_order_header_df.append(new_sales_order_header, ignore_index=True)\n",
    "\n",
    "        n_lines = randint(1, 5)\n",
    "        for line_number in range(n_lines):\n",
    "            new_sales_order_detail = {\n",
    "                \"SalesOrderID\": start_sales_order_id + i,\n",
    "                \"SalesOrderLineNumber\": line_number + 1,\n",
    "                \"ProductKey\": randint(1, 1000),\n",
    "                \"Qty\": randint(1, 20),\n",
    "                \"ModifiedDate\": datetime.now()\n",
    "            }\n",
    "            sales_order_detail_df = sales_order_detail_df.append(new_sales_order_detail, ignore_index=True)\n",
    "\n",
    "    # Save DataFrames to CSV files\n",
    "    customers_df.to_csv(f'customers_{process_id}.csv', index=False)\n",
    "    sales_order_header_df.to_csv(f'sales_order_header_{process_id}.csv', index=False)\n",
    "    sales_order_detail_df.to_csv(f'sales_order_detail_{process_id}.csv', index=False)\n",
    "\n",
    "def main(\n",
    "    n_orders: int, \n",
    "    order_date: str, \n",
    "    max_customer_id: int, \n",
    "    max_sales_order_id: int, \n",
    "    n_processes: int\n",
    "):\n",
    "    n_orders_per_process = n_orders // n_processes\n",
    "    max_customer_id_shared = Value('i', max_customer_id)\n",
    "    processes = []\n",
    "    lock = Lock()\n",
    "    \n",
    "    for i in range(n_processes):\n",
    "        start_sales_order_id = max_sales_order_id + i * n_orders_per_process\n",
    "        args = (n_orders_per_process, order_date, max_customer_id_shared, start_sales_order_id, lock, i)\n",
    "        p = Process(target=generate_sales_data, args=args)\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    \n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\n",
    "        n_orders=100, \n",
    "        order_date=\"2024-05-21\", \n",
    "        max_customer_id=1000, \n",
    "        max_sales_order_id=5000, \n",
    "        n_processes=4\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalesOrderID</th>\n",
       "      <th>OrderDate</th>\n",
       "      <th>CustomerID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>asd</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SalesOrderID OrderDate  CustomerID\n",
       "0             1       asd           3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_order_header_df = pd.DataFrame(columns=[\"SalesOrderID\", \"OrderDate\", \"CustomerID\"])\n",
    "new_sales_order_header = {\n",
    "            \"SalesOrderID\": 1,\n",
    "            \"OrderDate\": \"asd\",\n",
    "            \"CustomerID\": 3\n",
    "        }\n",
    "sales_order_header_df.loc[len(sales_order_header_df)] = new_sales_order_header\n",
    "sales_order_header_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "from multiprocessing import Process, Value, Lock\n",
    "from random import randint\n",
    "\n",
    "def generate_sales_data(\n",
    "    n_orders_per_process, \n",
    "    order_date, \n",
    "    max_customer_id_shared,\n",
    "    start_sales_order_id,\n",
    "    lock,\n",
    "    process_id\n",
    "):\n",
    "    fake = Faker()\n",
    "    # Create dataframes\n",
    "    customers_df = pd.DataFrame(columns=[\"CustomerID\", \"Name\", \"Email\", \"Age\", \"ModifiedDate\"])\n",
    "    sales_order_header_df = pd.DataFrame(columns=[\"SalesOrderID\", \"OrderDate\", \"CustomerID\"])\n",
    "    sales_order_detail_df = pd.DataFrame(columns=[\"SalesOrderID\", \"SalesOrderLineNumber\", \"ProductKey\", \"Qty\", \"ModifiedDate\"])\n",
    "    \n",
    "    for i in range(n_orders_per_process):\n",
    "        if randint(1, 10) > 8:\n",
    "            lock.acquire()\n",
    "            max_customer_id_shared.value += 1\n",
    "            curr_customer_id = max_customer_id_shared.value\n",
    "            lock.release()\n",
    "            # Generate customer infor\n",
    "            # Append new customer to Customer Dataframe\n",
    "            new_customer = {\n",
    "                \"CustomerID\": curr_customer_id,\n",
    "                \"Name\": fake.name(),\n",
    "                \"Email\": fake.email(),\n",
    "                \"Age\": randint(18, 90),\n",
    "                \"ModifiedDate\": order_date\n",
    "            }\n",
    "            customers_df.loc[len(customers_df)] = new_customer\n",
    "        else:\n",
    "            curr_customer_id = randint(1, max_customer_id_shared.value)\n",
    "        # Generate SalesOrderHeader row\n",
    "        # Append new row to SalesOrderHeader DATAFRAME\n",
    "        new_sales_order_header = {\n",
    "            \"SalesOrderID\": start_sales_order_id + i,\n",
    "            \"OrderDate\": order_date,\n",
    "            \"CustomerID\": curr_customer_id\n",
    "        }\n",
    "        sales_order_header_df.loc[len(sales_order_header_df)] = new_sales_order_header\n",
    "        \n",
    "        # Generate SalesOrderHeader row\n",
    "        # Append new row to SalesOrderHeader DATAFRAME\n",
    "        n_lines = randint(1, 10)\n",
    "        for line_number in range(n_lines):\n",
    "            new_sales_order_detail = {\n",
    "                \"SalesOrderID\": start_sales_order_id + i,\n",
    "                \"SalesOrderLineNumber\": line_number + 1,\n",
    "                \"ProductKey\": randint(1, 606),\n",
    "                \"Qty\": randint(1, 20),\n",
    "                \"ModifiedDate\": order_date\n",
    "            }\n",
    "            sales_order_detail_df.loc[len(sales_order_detail_df)] = new_sales_order_detail\n",
    "\n",
    "        # Save DataFrames to CSV files\n",
    "        customers_df.to_csv(f'data/customers_{process_id}.csv', index=False)\n",
    "        sales_order_header_df.to_csv(f'data/sales_order_header_{process_id}.csv', index=False)\n",
    "        sales_order_detail_df.to_csv(f'data/sales_order_detail_{process_id}.csv', index=False)\n",
    "\n",
    "def main(\n",
    "    n_orders: int, \n",
    "    order_date: str, \n",
    "    max_customer_id, \n",
    "    max_sales_order_id, \n",
    "    n_processes: int\n",
    "):\n",
    "    n_orders_per_process = n_orders // n_processes\n",
    "    max_customer_id_shared = Value('i', max_customer_id)\n",
    "    processes = []\n",
    "    lock = Lock()\n",
    "    for i in range(n_processes):\n",
    "        start_sales_order_id = max_sales_order_id + i * n_orders_per_process\n",
    "        args = (n_orders_per_process, order_date, max_customer_id_shared, start_sales_order_id, lock, i)\n",
    "        p = Process(target=generate_sales_data, args=args)\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    \n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "main(\n",
    "    n_orders=10000,\n",
    "    order_date=\"2022-10-15\",\n",
    "    max_customer_id=100,\n",
    "    max_sales_order_id=20000,\n",
    "    n_processes=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\n",
    "    n_orders=100000,\n",
    "    order_date=\"2022-10-15\",\n",
    "    max_customer_id=100,\n",
    "    max_sales_order_id=20000,\n",
    "    n_processes=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "from multiprocessing import Process, Value, Lock, Manager\n",
    "from random import randint\n",
    "import os\n",
    "\n",
    "def generate_sales_data(\n",
    "    n_orders_per_process, \n",
    "    order_date, \n",
    "    max_customer_id_shared,\n",
    "    start_sales_order_id,\n",
    "    lock,\n",
    "    process_id,\n",
    "    result_queue\n",
    "):\n",
    "    fake = Faker()\n",
    "    customer_data = []\n",
    "    sales_order_header_data = []\n",
    "    sales_order_detail_data = []\n",
    "    \n",
    "    for i in range(n_orders_per_process):\n",
    "        if randint(1, 10) > 8:\n",
    "            lock.acquire()\n",
    "            max_customer_id_shared.value += 1\n",
    "            curr_customer_id = max_customer_id_shared.value\n",
    "            lock.release()\n",
    "            new_customer = (\n",
    "                curr_customer_id,\n",
    "                fake.name(),\n",
    "                fake.email(),\n",
    "                randint(18, 90),\n",
    "                order_date\n",
    "            )\n",
    "            customer_data.append(new_customer)\n",
    "        else:\n",
    "            curr_customer_id = randint(1, max_customer_id_shared.value)\n",
    "        \n",
    "        new_sales_order_header = (\n",
    "            start_sales_order_id + i,\n",
    "            order_date,\n",
    "            curr_customer_id\n",
    "        )\n",
    "        sales_order_header_data.append(new_sales_order_header)\n",
    "        \n",
    "        n_lines = randint(1, 10)\n",
    "        for line_number in range(n_lines):\n",
    "            new_sales_order_detail = (\n",
    "                start_sales_order_id + i,\n",
    "                line_number + 1,\n",
    "                randint(1, 606),\n",
    "                randint(1, 20),\n",
    "                order_date\n",
    "            )\n",
    "            sales_order_detail_data.append(new_sales_order_detail)\n",
    "    \n",
    "    result_queue.put((process_id, customer_data, sales_order_header_data, sales_order_detail_data))\n",
    "\n",
    "def save_to_csv(result_queue, n_processes):\n",
    "    customers_data = []\n",
    "    sales_order_header_data = []\n",
    "    sales_order_detail_data = []\n",
    "    \n",
    "    for _ in range(n_processes):\n",
    "        process_id, customer_data, sales_order_header_data, sales_order_detail_data = result_queue.get()\n",
    "        customers_data.extend(customer_data)\n",
    "        sales_order_header_data.extend(sales_order_header_data)\n",
    "        sales_order_detail_data.extend(sales_order_detail_data)\n",
    "    \n",
    "    customers_df = pd.DataFrame(customers_data, columns=[\"CustomerID\", \"Name\", \"Email\", \"Age\", \"ModifiedDate\"])\n",
    "    sales_order_header_df = pd.DataFrame(sales_order_header_data, columns=[\"SalesOrderID\", \"OrderDate\", \"CustomerID\"])\n",
    "    sales_order_detail_df = pd.DataFrame(sales_order_detail_data, columns=[\"SalesOrderID\", \"SalesOrderLineNumber\", \"ProductKey\", \"Qty\", \"ModifiedDate\"])\n",
    "\n",
    "    customers_df.to_csv('data/customers.csv', index=False)\n",
    "    sales_order_header_df.to_csv('data/sales_order_header.csv', index=False)\n",
    "    sales_order_detail_df.to_csv('data/sales_order_detail.csv', index=False)\n",
    "\n",
    "def main(\n",
    "    n_orders: int, \n",
    "    order_date: str, \n",
    "    max_customer_id, \n",
    "    max_sales_order_id, \n",
    "    n_processes: int\n",
    "):\n",
    "    n_orders_per_process = n_orders // n_processes\n",
    "    max_customer_id_shared = Value('i', max_customer_id)\n",
    "    manager = Manager()\n",
    "    result_queue = manager.Queue()\n",
    "    processes = []\n",
    "    lock = Lock()\n",
    "    \n",
    "    for i in range(n_processes):\n",
    "        start_sales_order_id = max_sales_order_id + i * n_orders_per_process\n",
    "        args = (n_orders_per_process, order_date, max_customer_id_shared, start_sales_order_id, lock, i, result_queue)\n",
    "        p = Process(target=generate_sales_data, args=args)\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    \n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    \n",
    "    save_to_csv(result_queue, n_processes)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "    \n",
    "    main(\n",
    "        n_orders=100000,\n",
    "        order_date=\"2022-10-15\",\n",
    "        max_customer_id=100,\n",
    "        max_sales_order_id=20000,\n",
    "        n_processes=8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 3 start sending generated data into the queue.\n",
      "\n",
      "Process 1 start sending generated data into the queue.\n",
      "\n",
      "Process 7 start sending generated data into the queue.\n",
      "\n",
      "Process 4 start sending generated data into the queue.\n",
      "\n",
      "Process 1 finish sending generated data into the queue.\n",
      "Process 0 start sending generated data into the queue.\n",
      "\n",
      "\n",
      "Process 2 start sending generated data into the queue.\n",
      "\n",
      "Process 3 finish sending generated data into the queue.\n",
      "\n",
      "Process 5 start sending generated data into the queue.\n",
      "\n",
      "Process 6 start sending generated data into the queue.\n",
      "\n",
      "Process 0 finish sending generated data into the queue.\n",
      "\n",
      "Process 7 finish sending generated data into the queue.\n",
      "\n",
      "Process 4 finish sending generated data into the queue.\n",
      "\n",
      "Process 5 finish sending generated data into the queue.\n",
      "\n",
      "Process 6 finish sending generated data into the queue.\n",
      "\n",
      "Process 2 finish sending generated data into the queue.\n",
      "\n",
      "Start writing data to .csv files\n",
      "Finish writing data to .csv files\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "from multiprocessing import Process, Value, Lock, Manager\n",
    "from random import randint\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "\n",
    "def generate_sales_data(\n",
    "    n_orders_per_process, \n",
    "    order_date, \n",
    "    max_customer_id_shared,\n",
    "    start_sales_order_id,\n",
    "    lock,\n",
    "    process_id,\n",
    "    result_queue\n",
    "):       \n",
    "    # print(f\"Process {process_id} start generating data.\")\n",
    "    fake = Faker()\n",
    "    customer_data = []\n",
    "    sales_order_header_data = []\n",
    "    sales_order_detail_data = []\n",
    "    \n",
    "    for i in range(n_orders_per_process):\n",
    "        if randint(1, 10) > 8:\n",
    "            lock.acquire()\n",
    "            max_customer_id_shared.value += 1\n",
    "            curr_customer_id = max_customer_id_shared.value\n",
    "            lock.release()\n",
    "            new_customer = (\n",
    "                curr_customer_id,\n",
    "                fake.name(),\n",
    "                fake.email(),\n",
    "                randint(18, 90),\n",
    "                order_date\n",
    "            )\n",
    "            customer_data.append(new_customer)\n",
    "        else:\n",
    "            curr_customer_id = randint(1, max_customer_id_shared.value)\n",
    "        \n",
    "        new_sales_order_header = (\n",
    "            start_sales_order_id + i,\n",
    "            order_date,\n",
    "            curr_customer_id\n",
    "        )\n",
    "        sales_order_header_data.append(new_sales_order_header)\n",
    "        \n",
    "        n_lines = randint(1, 10)\n",
    "        for line_number in range(n_lines):\n",
    "            new_sales_order_detail = (\n",
    "                start_sales_order_id + i,\n",
    "                line_number + 1,\n",
    "                randint(1, 606),\n",
    "                randint(1, 20),\n",
    "                order_date\n",
    "            )\n",
    "            sales_order_detail_data.append(new_sales_order_detail)\n",
    "    \n",
    "    # Put data into the Queue\n",
    "    result_queue.put((process_id, customer_data, sales_order_header_data, sales_order_detail_data))\n",
    "\n",
    "def save_to_csv(result_queue, n_processes):\n",
    "    print(\"Start writing data to .csv files\")\n",
    "    customers_data = []\n",
    "    all_sales_order_header_data = []\n",
    "    all_sales_order_detail_data = []\n",
    "    \n",
    "    for _ in range(n_processes):\n",
    "        process_id, customer_data, sales_order_header_data, sales_order_detail_data = result_queue.get()\n",
    "        customers_data.extend(customer_data)\n",
    "        all_sales_order_header_data.extend(sales_order_header_data)\n",
    "        all_sales_order_detail_data.extend(sales_order_detail_data)\n",
    "    \n",
    "    customers_df = pd.DataFrame(customers_data, columns=[\"CustomerID\", \"Name\", \"Email\", \"Age\", \"ModifiedDate\"])\n",
    "    customers_df.sort_values(by='CustomerID', ascending=True, inplace=True)\n",
    "    sales_order_header_df = pd.DataFrame(all_sales_order_header_data, columns=[\"SalesOrderID\", \"OrderDate\", \"CustomerID\"])\n",
    "    sales_order_header_df.sort_values(by='SalesOrderID', ascending=True, inplace=True)\n",
    "    sales_order_detail_df = pd.DataFrame(all_sales_order_detail_data, columns=[\"SalesOrderID\", \"SalesOrderLineNumber\", \"ProductKey\", \"Qty\", \"ModifiedDate\"])\n",
    "    sales_order_detail_df.sort_values(by=['SalesOrderID', 'SalesOrderLineNumber'], ascending=[True, True], inplace=True)\n",
    "    \n",
    "    customers_df.to_csv('data/customers.csv', index=False, sep=',', quotechar='\"', lineterminator='\\n', quoting=csv.QUOTE_ALL)\n",
    "    sales_order_header_df.to_csv('data/sales_order_header.csv', index=False, sep=',', quotechar='\"', lineterminator='\\n', quoting=csv.QUOTE_ALL)\n",
    "    sales_order_detail_df.to_csv('data/sales_order_detail.csv', index=False, sep=',', quotechar='\"', lineterminator='\\n', quoting=csv.QUOTE_ALL)\n",
    "    print(\"Finish writing data to .csv files\")\n",
    "\n",
    "def main(\n",
    "    n_orders: int, \n",
    "    order_date: str, \n",
    "    max_customer_id, \n",
    "    max_sales_order_id, \n",
    "    n_processes: int\n",
    "):\n",
    "    n_orders_per_process = n_orders // n_processes\n",
    "    remainder_orders = n_orders % n_processes\n",
    "    max_customer_id_shared = Value('i', max_customer_id)\n",
    "    manager = Manager()\n",
    "    result_queue = manager.Queue()\n",
    "    processes = []\n",
    "    lock = Lock()\n",
    "    \n",
    "    for i in range(n_processes):\n",
    "        additional_order = 1 if i < remainder_orders else 0\n",
    "        start_sales_order_id = max_sales_order_id + i * (n_orders_per_process + additional_order)\n",
    "        args = (n_orders_per_process + additional_order, order_date, max_customer_id_shared, start_sales_order_id, lock, i, result_queue)\n",
    "        p = Process(target=generate_sales_data, args=args)\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    \n",
    "    for p in processes:\n",
    "        p.join()\n",
    "    \n",
    "    save_to_csv(result_queue, n_processes)\n",
    "\n",
    "\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')\n",
    "\n",
    "main(\n",
    "    n_orders=100000,\n",
    "    order_date=\"2022-11-06\",\n",
    "    max_customer_id=47029,\n",
    "    max_sales_order_id=154733,\n",
    "    n_processes=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to MySQL database is successful\n",
      "information_schema\n",
      "performance_schema\n",
      "retail_db\n",
      "MySQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "from src.utils.db_connection import DBConnection\n",
    "\n",
    "conn = DBConnection()\n",
    "try:\n",
    "    conn.connect()\n",
    "    conn.list_schemas()\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to MySQL database is successful\n",
      "Start loading csv file from /home/phinguyen/ETL_Pipeline_with_Spark_01/notebooks/data/customers.csv into table Customer\n",
      "Finish loading csv file from /home/phinguyen/ETL_Pipeline_with_Spark_01/notebooks/data/customers.csv into table Customer\n",
      "MySQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "def load_csv_to_database(csv_file_path: str, table_name: str, conn: DBConnection):\n",
    "    load_query = f\"\"\"\n",
    "    LOAD DATA LOCAL INFILE '{csv_file_path}'\n",
    "    INTO TABLE {table_name}\n",
    "    FIELDS TERMINATED BY ','  -- specify the delimiter used in your CSV file\n",
    "    ENCLOSED BY '\"'           -- specify if the fields are enclosed by a specific character\n",
    "    LINES TERMINATED BY '\\n'  -- specify the line terminator\n",
    "    IGNORE 1 LINES            -- skip the header row if your CSV has a header\n",
    "    \"\"\"\n",
    "    print(f\"Start loading csv file from {csv_file_path} into table {table_name}\")\n",
    "    conn.execute_query(load_query)\n",
    "    print(f\"Finish loading csv file from {csv_file_path} into table {table_name}\")\n",
    "\n",
    "try:\n",
    "    conn.connect()\n",
    "    load_csv_to_database(\"/home/phinguyen/ETL_Pipeline_with_Spark_01/notebooks/data/customers.csv\", \"Customer\", conn)\n",
    "except Exception as e:\n",
    "    raise Exception(e)\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to MySQL database is successful\n",
      "Start loading csv file from /home/phinguyen/ETL_Pipeline_with_Spark_01/notebooks/data/sales_order_header.csv into table SalesOrderHeader\n",
      "Finish loading csv file from /home/phinguyen/ETL_Pipeline_with_Spark_01/notebooks/data/sales_order_header.csv into table SalesOrderHeader\n",
      "MySQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    conn.connect()\n",
    "    load_csv_to_database(\"/home/phinguyen/ETL_Pipeline_with_Spark_01/notebooks/data/sales_order_header.csv\", \n",
    "                         \"SalesOrderHeader\", \n",
    "                         conn)\n",
    "except Exception as e:\n",
    "    raise Exception(e)\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to MySQL database is successful\n",
      "Start loading csv file from /home/phinguyen/ETL_Pipeline_with_Spark_01/notebooks/data/sales_order_detail.csv into table SalesOrderDetail\n",
      "Finish loading csv file from /home/phinguyen/ETL_Pipeline_with_Spark_01/notebooks/data/sales_order_detail.csv into table SalesOrderDetail\n",
      "MySQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    conn.connect()\n",
    "    load_csv_to_database(\"/home/phinguyen/ETL_Pipeline_with_Spark_01/notebooks/data/sales_order_detail.csv\", \n",
    "                         \"SalesOrderDetail\", \n",
    "                         conn)\n",
    "except Exception as e:\n",
    "    raise Exception(e)\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to MySQL database is successful\n",
      "MySQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    conn.connect()\n",
    "    max_customer_id_query = \"SELECT MAX(CustomerId) FROM Customer\"\n",
    "    max_sales_order_id_query = \"SELECT MAX(SalesOrderId) FROM SalesOrderHeader\"\n",
    "    max_customer_id = conn.execute_query(max_customer_id_query)\n",
    "    max_sales_order_id = conn.execute_query(max_sales_order_id_query)\n",
    "except Exception as e:\n",
    "    raise Exception(e)\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67180"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_customer_id[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
