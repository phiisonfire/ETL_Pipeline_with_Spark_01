{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "906b6bff-f5de-4ce9-840e-989faa0e99fc",
   "metadata": {},
   "source": [
    "# ETL Pipeline with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef62583-dba2-4130-86ce-c6183e48cff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 02:35:47 WARN Utils: Your hostname, desktop resolves to a loopback address: 127.0.1.1; using 172.18.52.176 instead (on interface eth0)\n",
      "24/05/26 02:35:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/05/26 02:35:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.18.52.176:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ETL_Pipeline_Testing</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7faefc3ecec0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create Spark session with mysql connector jar\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local') \\\n",
    "    .config(\"spark.jars\", \"/home/phinguyen/lib/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .appName('ETL_Pipeline_Testing') \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "260c3df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor Memory: None\n",
      "Driver Memory: None\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "\n",
    "# Initialize SparkConf\n",
    "conf = SparkConf()\n",
    "\n",
    "# Get the executor memory\n",
    "executor_memory = conf.get(\"spark.executor.memory\")\n",
    "print(f\"Executor Memory: {executor_memory}\")\n",
    "\n",
    "# Get the driver memory\n",
    "driver_memory = conf.get(\"spark.driver.memory\")\n",
    "print(f\"Driver Memory: {driver_memory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49271abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor Memory: None\n",
      "Driver Memory: None\n"
     ]
    }
   ],
   "source": [
    "# Assuming spark is your SparkSession\n",
    "spark_context = spark.sparkContext\n",
    "\n",
    "# Get the executor memory\n",
    "executor_memory = spark_context.getConf().get(\"spark.executor.memory\")\n",
    "print(f\"Executor Memory: {executor_memory}\")\n",
    "\n",
    "# Get the driver memory\n",
    "driver_memory = spark_context.getConf().get(\"spark.driver.memory\")\n",
    "print(f\"Driver Memory: {driver_memory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6981adf8-4756-4c0b-a508-994d6b698c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table Customer from mysql\n",
    "customer_df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/retail_db\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"Customer\") \\\n",
    "    .option(\"user\", \"admin\") \\\n",
    "    .option(\"password\", \"adminpassword\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "883bbafe-5561-4bd6-aace-cdb3eea4945b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[CustomerID: int, Name: string, Email: string, Age: int, ModifiedDate: timestamp]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "062a159f-7bd2-45e8-baca-74c5de4939d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "customer_df.createOrReplaceTempView(\"mySQL_customer_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a2a33b3-2d99-4d1b-8cd4-cc90594e91b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------------------+---+-------------------+\n",
      "|CustomerID|            Name|               Email|Age|       ModifiedDate|\n",
      "+----------+----------------+--------------------+---+-------------------+\n",
      "|         1|Christina Savage| vgreene@example.com| 59|2022-10-20 00:00:00|\n",
      "|         2|   Zachary Green|costajohn@example...| 85|2022-10-20 00:00:00|\n",
      "|         3|   Andrea Wilson|  john53@example.org| 73|2022-10-20 00:00:00|\n",
      "+----------+----------------+--------------------+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run SQL query on the view\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        *\n",
    "    FROM mySQL_customer_tbl\n",
    "    LIMIT 3\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17a32fdc-4071-474a-9b3c-51e6d4dbb56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cus = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        MAX(CustomerID)\n",
    "    FROM mySQL_customer_tbl\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60c7334d-0fe1-44fa-afac-f20ee1c09a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103358"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_cus.collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33f3cddb-26e9-415e-bd14-ffe0376aea6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103358"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df.selectExpr(\"MAX(CustomerID)\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7da3b720-016f-4de8-9fbb-c85dee2af670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+----------+----------------+--------------------+---+-------------------+----+-----+---+\n",
      "|CustomerID|            Name|               Email|Age|       ModifiedDate|year|month|day|\n",
      "+----------+----------------+--------------------+---+-------------------+----+-----+---+\n",
      "|         1|Christina Savage| vgreene@example.com| 59|2022-10-20 00:00:00|2022|   10| 20|\n",
      "|         2|   Zachary Green|costajohn@example...| 85|2022-10-20 00:00:00|2022|   10| 20|\n",
      "|         3|   Andrea Wilson|  john53@example.org| 73|2022-10-20 00:00:00|2022|   10| 20|\n",
      "+----------+----------------+--------------------+---+-------------------+----+-----+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transform the spark dataframe\n",
    "customer_output_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        *,\n",
    "        YEAR(ModifiedDate) AS year,\n",
    "        MONTH(ModifiedDate) AS month,\n",
    "        DAY(ModifiedDate) AS day\n",
    "    FROM mySQL_customer_tbl\n",
    "\"\"\")\n",
    "print(type(customer_output_df))\n",
    "customer_output_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66a3d942-acc0-4136-92f4-149d41b11d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://localhost:9000/tmp\n",
      "hdfs://localhost:9000/user\n"
     ]
    }
   ],
   "source": [
    "from py4j.java_gateway import java_import\n",
    "\n",
    "# Import necessary classes from Java\n",
    "java_import(spark._jvm, 'org.apache.hadoop.fs.FileSystem')\n",
    "java_import(spark._jvm, 'org.apache.hadoop.fs.Path')\n",
    "java_import(spark._jvm, 'org.apache.hadoop.fs.FileStatus')\n",
    "\n",
    "# Get the Hadoop configuration\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration() # current fs.defaultFS is 'file:///' which pointing to local file system.\n",
    "hadoop_conf.set(\"fs.defaultFS\", \"hdfs://localhost:9000\") # This ensures that Spark uses HDFS instead of the local file system.\n",
    "\n",
    "# Create a FileSystem object\n",
    "fs = spark._jvm.FileSystem.get(hadoop_conf)\n",
    "\n",
    "# Define the HDFS directory path\n",
    "hdfs_directory_path = spark._jvm.Path(\"/\")\n",
    "\n",
    "# List the contents of the directory\n",
    "file_statuses = fs.listStatus(hdfs_directory_path)\n",
    "\n",
    "# Iterate through the statuses and print them\n",
    "for file_status in file_statuses:\n",
    "    print(file_status.getPath())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f388b87-eafb-404f-92a1-0ffe5dd63859",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o114.listStatus.\n: java.io.FileNotFoundException: File /datalake does not exist.\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:1104)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:147)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$24.doCall(DistributedFileSystem.java:1175)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$24.doCall(DistributedFileSystem.java:1172)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:1182)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m datalake_directory_path \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/datalake\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# List the contents of the directory\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m file_statuses \u001b[38;5;241m=\u001b[39m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistStatus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatalake_directory_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Iterate through the statuses and print them\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_status \u001b[38;5;129;01min\u001b[39;00m file_statuses:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o114.listStatus.\n: java.io.FileNotFoundException: File /datalake does not exist.\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:1104)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:147)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$24.doCall(DistributedFileSystem.java:1175)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$24.doCall(DistributedFileSystem.java:1172)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:1182)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "datalake_directory_path = spark._jvm.Path(\"/datalake\")\n",
    "\n",
    "# List the contents of the directory\n",
    "file_statuses = fs.listStatus(datalake_directory_path)\n",
    "\n",
    "# Iterate through the statuses and print them\n",
    "for file_status in file_statuses:\n",
    "    print(file_status.getPath())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b448016-4a15-452c-bebe-504d36377360",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_directory_path = spark._jvm.Path(\"/datalake/Customer\")\n",
    "# Check if the directory exists\n",
    "exists = fs.exists(customer_directory_path)\n",
    "exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeab639-3c93-405d-8969-96a6959ab720",
   "metadata": {},
   "outputs": [],
   "source": [
    "if exists:\n",
    "    # Convert Path to string\n",
    "    customer_directory_str = str(customer_directory_path.toString())\n",
    "    print(customer_directory_str)\n",
    "    datalake_customer_df = spark.read.parquet(customer_directory_str)\n",
    "    datalake_customer_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdb08d0-1d71-43ea-b6fb-bd830ffcfdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalake_customer_df.selectExpr(\"max(CustomerID)\").collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed9bb6b-9319-48f5-98ec-4920eecdcc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(spark))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9e0c2c",
   "metadata": {},
   "source": [
    "# Spark OOM handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab9ff1f-1e54-4e6a-b90c-c2e8477f2b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "import argparse\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86e33454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logs_dir = \"logs\"\n",
    "if not os.path.exists(logs_dir):\n",
    "    os.makedirs(logs_dir)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(logs_dir, \"ingestion.log\"),\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Add ANSI escape sequences for bold yellow text\n",
    "BOLD_YELLOW = \"\\033[1m\\033[93m\"\n",
    "RESET = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9670875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hdfs_FileSystem_obj(spark: SparkSession, hdfs_uri: str):\n",
    "    from py4j.java_gateway import java_import\n",
    "    \n",
    "    # Import necessary classes from Java\n",
    "    java_import(spark._jvm, 'org.apache.hadoop.fs.FileSystem')\n",
    "    java_import(spark._jvm, 'org.apache.hadoop.fs.Path')\n",
    "    java_import(spark._jvm, 'org.apache.hadoop.fs.FileStatus')\n",
    "    \n",
    "    # Get the Hadoop configuration\n",
    "    hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "    hadoop_conf.set(\"fs.defaultFS\", hdfs_uri)\n",
    "\n",
    "    # Create a FileSystem object\n",
    "    return spark._jvm.FileSystem.get(hadoop_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "febf02ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/05/23 04:16:33 WARN Utils: Your hostname, Phii resolves to a loopback address: 127.0.1.1; using 172.19.26.206 instead (on interface eth0)\n",
      "24/05/23 04:16:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Exception in thread \"main\" org.apache.spark.SparkException: When running with master 'yarn' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.\n",
      "\tat org.apache.spark.deploy.SparkSubmitArguments.error(SparkSubmitArguments.scala:650)\n",
      "\tat org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:281)\n",
      "\tat org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:237)\n",
      "\tat org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:122)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$3.<init>(SparkSubmit.scala:1103)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments(SparkSubmit.scala:1103)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m\n\u001b[1;32m     18\u001b[0m mysql_database_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMYSQL_DATABASE\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Create Spark session\u001b[39;00m\n\u001b[1;32m     21\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.cores\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m4g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.cores\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.jars\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/phinguyen/lib/mysql-connector-j-8.0.33.jar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myarn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIngestion - from OLTP Database (MySQL) to DataLake (Hadoop HDFS)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyspark/lib/python3.10/site-packages/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyspark/lib/python3.10/site-packages/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/miniconda3/envs/pyspark/lib/python3.10/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/pyspark/lib/python3.10/site-packages/pyspark/context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m~/miniconda3/envs/pyspark/lib/python3.10/site-packages/pyspark/java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "table_name = \"SalesOrderDetail\"\n",
    "\n",
    "# Define table-specific details\n",
    "table_details = {\n",
    "    \"Customer\": {\"primary_col\": \"CustomerID\", \"date_col\": \"ModifiedDate\"},\n",
    "    \"SalesOrderHeader\": {\"primary_col\": \"SalesOrderID\", \"date_col\": \"OrderDate\"},\n",
    "    \"SalesOrderDetail\": {\"primary_col\": \"SalesOrderID\", \"date_col\": \"ModifiedDate\"}\n",
    "}\n",
    "\n",
    "primary_col = table_details[table_name][\"primary_col\"]\n",
    "date_col = table_details[table_name][\"date_col\"]\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "mysql_user = os.getenv('MYSQL_USER')\n",
    "mysql_password = os.getenv('MYSQL_PASSWORD')\n",
    "mysql_host = os.getenv('MYSQL_HOST')\n",
    "mysql_database_name = os.getenv('MYSQL_DATABASE')\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.jars\", \"/home/phinguyen/lib/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"Ingestion - from OLTP Database (MySQL) to DataLake (Hadoop HDFS)\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd789ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fs = get_hdfs_FileSystem_obj(spark, hdfs_uri=\"hdfs://localhost:9900\")\n",
    "table_dir_str = \"/datalake/\" + table_name\n",
    "hdfs_table_dir_path = spark._jvm.Path(table_dir_str)\n",
    "\n",
    "# Check for existing data in HDFS\n",
    "if fs.exists(hdfs_table_dir_path):\n",
    "    hdfs_df = spark.read.parquet(table_dir_str)\n",
    "    hdfs_df.createOrReplaceTempView(\"hdfs_table\")\n",
    "    latest_record = spark.sql(f\"SELECT MAX({primary_col}) FROM hdfs_table\").collect()[0][0]\n",
    "else:\n",
    "    latest_record = 0\n",
    "\n",
    "# Load new data from MySQL\n",
    "mysql_df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:mysql://{mysql_host}:3306/{mysql_database_name}\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", table_name) \\\n",
    "    .option(\"user\", mysql_user) \\\n",
    "    .option(\"password\", mysql_password) \\\n",
    "    .load()\n",
    "\n",
    "mysql_df.createOrReplaceTempView(\"mysql_table\")\n",
    "output_df = spark.sql(f\"\"\"\n",
    "                        SELECT\n",
    "                            *,\n",
    "                            YEAR({date_col}) AS year,\n",
    "                            MONTH({date_col}) AS month,\n",
    "                            DAY({date_col}) AS day\n",
    "                        FROM mysql_table\n",
    "                        WHERE {primary_col} > {latest_record}\n",
    "                        \"\"\")\n",
    "\n",
    "new_records_cnt = output_df.selectExpr(f\"COUNT({primary_col})\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cd0af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7783525"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_records_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e94a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17397534102201462"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_rows = 7783525\n",
    "estimated_row_size_bytes = 24\n",
    "total_memory_consumption_bytes = total_rows * estimated_row_size_bytes\n",
    "total_memory_consumption_gb = total_memory_consumption_bytes / (1024**3)  # Convert bytes to GB\n",
    "total_memory_consumption_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb27da29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 04:09:50 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "24/05/23 04:09:52 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "24/05/23 04:09:55 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "24/05/23 04:09:57 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "24/05/23 04:10:00 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "24/05/23 04:10:02 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "24/05/23 04:10:06 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "24/05/23 04:10:09 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "24/05/23 04:10:12 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "24/05/23 04:10:14 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "24/05/23 04:10:17 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "24/05/23 04:10:19 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "24/05/23 04:10:22 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "output_df.write.partitionBy(\"year\", \"month\", \"day\").mode(\"append\").parquet(table_dir_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b42295fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "customer_df = spark.read.parquet(\"/datalake/Customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3303624-cc6a-4baa-a2d3-f2ea9939f2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.createOrReplaceTempView(\"customer_datalake_view\")\n",
    "count_last_date = spark.sql(\"\"\"\n",
    "    SELECT COUNT(1)\n",
    "    FROM customer_datalake_view\n",
    "    WHERE ModifiedDate = '2024-02-13'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f356e48-7913-459c-9ef7-c9e5de61b45d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[count(1): bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_last_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec4919b1-ca0a-4427-a150-e19e4599cd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  200265|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_last_date.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c8a94cc-e553-4064-b0fa-1395158991f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  3|\n",
      "|  5|\n",
      "|  7|\n",
      "|  9|\n",
      "| 11|\n",
      "| 13|\n",
      "| 15|\n",
      "| 17|\n",
      "| 19|\n",
      "| 21|\n",
      "| 23|\n",
      "| 25|\n",
      "| 27|\n",
      "| 29|\n",
      "| 31|\n",
      "| 33|\n",
      "| 35|\n",
      "| 37|\n",
      "| 39|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.range(1, 100, 2)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b41a27e-3a85-44ff-9266-5343c37034b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|5id|\n",
      "+---+\n",
      "|  5|\n",
      "| 15|\n",
      "| 25|\n",
      "| 35|\n",
      "| 45|\n",
      "| 55|\n",
      "| 65|\n",
      "| 75|\n",
      "| 85|\n",
      "| 95|\n",
      "|105|\n",
      "|115|\n",
      "|125|\n",
      "|135|\n",
      "|145|\n",
      "|155|\n",
      "|165|\n",
      "|175|\n",
      "|185|\n",
      "|195|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.selectExpr(\"id * 5 as 5id\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36a93201-fd2f-4149-b9ca-c67175bd1640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(sum(id)=2500000000000)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = spark.range(2, 10000000, 2)\n",
    "df2 = spark.range(2, 10000000, 4)\n",
    "step1 = df1.repartition(5)\n",
    "step12 = df2.repartition(6)\n",
    "step2 = step1.selectExpr(\"id * 5 as id\")\n",
    "step3 = step2.join(step12, [\"id\"])\n",
    "step4 = step3.selectExpr(\"sum(id)\")\n",
    "step4.collect() # 2500000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "066dca67-b530-4852-8c04-9f29a00f9609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=true\n",
      "+- == Final Plan ==\n",
      "   *(7) HashAggregate(keys=[], functions=[sum(id#48L)])\n",
      "   +- ShuffleQueryStage 4\n",
      "      +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=342]\n",
      "         +- *(6) HashAggregate(keys=[], functions=[partial_sum(id#48L)])\n",
      "            +- *(6) Project [id#48L]\n",
      "               +- *(6) SortMergeJoin [id#48L], [id#42L], Inner\n",
      "                  :- *(4) Sort [id#48L ASC NULLS FIRST], false, 0\n",
      "                  :  +- AQEShuffleRead coalesced\n",
      "                  :     +- ShuffleQueryStage 2\n",
      "                  :        +- Exchange hashpartitioning(id#48L, 200), ENSURE_REQUIREMENTS, [plan_id=213]\n",
      "                  :           +- *(3) Project [(id#40L * 5) AS id#48L]\n",
      "                  :              +- ShuffleQueryStage 0\n",
      "                  :                 +- Exchange RoundRobinPartitioning(5), REPARTITION_BY_NUM, [plan_id=146]\n",
      "                  :                    +- *(1) Range (2, 10000000, step=2, splits=1)\n",
      "                  +- *(5) Sort [id#42L ASC NULLS FIRST], false, 0\n",
      "                     +- AQEShuffleRead coalesced\n",
      "                        +- ShuffleQueryStage 3\n",
      "                           +- Exchange hashpartitioning(id#42L, 200), ENSURE_REQUIREMENTS, [plan_id=158]\n",
      "                              +- ShuffleQueryStage 1\n",
      "                                 +- Exchange RoundRobinPartitioning(6), REPARTITION_BY_NUM, [plan_id=154]\n",
      "                                    +- *(2) Range (2, 10000000, step=4, splits=1)\n",
      "+- == Initial Plan ==\n",
      "   HashAggregate(keys=[], functions=[sum(id#48L)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=122]\n",
      "      +- HashAggregate(keys=[], functions=[partial_sum(id#48L)])\n",
      "         +- Project [id#48L]\n",
      "            +- SortMergeJoin [id#48L], [id#42L], Inner\n",
      "               :- Sort [id#48L ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(id#48L, 200), ENSURE_REQUIREMENTS, [plan_id=114]\n",
      "               :     +- Project [(id#40L * 5) AS id#48L]\n",
      "               :        +- Exchange RoundRobinPartitioning(5), REPARTITION_BY_NUM, [plan_id=104]\n",
      "               :           +- Range (2, 10000000, step=2, splits=1)\n",
      "               +- Sort [id#42L ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(id#42L, 200), ENSURE_REQUIREMENTS, [plan_id=115]\n",
      "                     +- Exchange RoundRobinPartitioning(6), REPARTITION_BY_NUM, [plan_id=107]\n",
      "                        +- Range (2, 10000000, step=4, splits=1)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "step4.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef173a2b-1d4e-4a85-b6f3-d75aa353b31d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
