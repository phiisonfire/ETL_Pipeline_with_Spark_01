{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "906b6bff-f5de-4ce9-840e-989faa0e99fc",
   "metadata": {},
   "source": [
    "# ETL Pipeline with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef62583-dba2-4130-86ce-c6183e48cff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/26 02:35:47 WARN Utils: Your hostname, desktop resolves to a loopback address: 127.0.1.1; using 172.18.52.176 instead (on interface eth0)\n",
      "24/05/26 02:35:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/05/26 02:35:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.18.52.176:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ETL_Pipeline_Testing</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7faefc3ecec0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create Spark session with mysql connector jar\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local') \\\n",
    "    .config(\"spark.jars\", \"/home/phinguyen/lib/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .appName('ETL_Pipeline_Testing') \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "260c3df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor Memory: None\n",
      "Driver Memory: None\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "\n",
    "# Initialize SparkConf\n",
    "conf = SparkConf()\n",
    "\n",
    "# Get the executor memory\n",
    "executor_memory = conf.get(\"spark.executor.memory\")\n",
    "print(f\"Executor Memory: {executor_memory}\")\n",
    "\n",
    "# Get the driver memory\n",
    "driver_memory = conf.get(\"spark.driver.memory\")\n",
    "print(f\"Driver Memory: {driver_memory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49271abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executor Memory: None\n",
      "Driver Memory: None\n"
     ]
    }
   ],
   "source": [
    "# Assuming spark is your SparkSession\n",
    "spark_context = spark.sparkContext\n",
    "\n",
    "# Get the executor memory\n",
    "executor_memory = spark_context.getConf().get(\"spark.executor.memory\")\n",
    "print(f\"Executor Memory: {executor_memory}\")\n",
    "\n",
    "# Get the driver memory\n",
    "driver_memory = spark_context.getConf().get(\"spark.driver.memory\")\n",
    "print(f\"Driver Memory: {driver_memory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6981adf8-4756-4c0b-a508-994d6b698c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get table Customer from mysql\n",
    "customer_df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/retail_db\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"Customer\") \\\n",
    "    .option(\"user\", \"admin\") \\\n",
    "    .option(\"password\", \"adminpassword\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "883bbafe-5561-4bd6-aace-cdb3eea4945b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[CustomerID: int, Name: string, Email: string, Age: int, ModifiedDate: timestamp]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "062a159f-7bd2-45e8-baca-74c5de4939d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SQL view\n",
    "customer_df.createOrReplaceTempView(\"mySQL_customer_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a2a33b3-2d99-4d1b-8cd4-cc90594e91b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------------------+---+-------------------+\n",
      "|CustomerID|            Name|               Email|Age|       ModifiedDate|\n",
      "+----------+----------------+--------------------+---+-------------------+\n",
      "|         1|Christina Savage| vgreene@example.com| 59|2022-10-20 00:00:00|\n",
      "|         2|   Zachary Green|costajohn@example...| 85|2022-10-20 00:00:00|\n",
      "|         3|   Andrea Wilson|  john53@example.org| 73|2022-10-20 00:00:00|\n",
      "+----------+----------------+--------------------+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run SQL query on the view\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        *\n",
    "    FROM mySQL_customer_tbl\n",
    "    LIMIT 3\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17a32fdc-4071-474a-9b3c-51e6d4dbb56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cus = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        MAX(CustomerID)\n",
    "    FROM mySQL_customer_tbl\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60c7334d-0fe1-44fa-afac-f20ee1c09a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103358"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_cus.collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33f3cddb-26e9-415e-bd14-ffe0376aea6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103358"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_df.selectExpr(\"MAX(CustomerID)\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7da3b720-016f-4de8-9fbb-c85dee2af670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+----------+----------------+--------------------+---+-------------------+----+-----+---+\n",
      "|CustomerID|            Name|               Email|Age|       ModifiedDate|year|month|day|\n",
      "+----------+----------------+--------------------+---+-------------------+----+-----+---+\n",
      "|         1|Christina Savage| vgreene@example.com| 59|2022-10-20 00:00:00|2022|   10| 20|\n",
      "|         2|   Zachary Green|costajohn@example...| 85|2022-10-20 00:00:00|2022|   10| 20|\n",
      "|         3|   Andrea Wilson|  john53@example.org| 73|2022-10-20 00:00:00|2022|   10| 20|\n",
      "+----------+----------------+--------------------+---+-------------------+----+-----+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transform the spark dataframe\n",
    "customer_output_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        *,\n",
    "        YEAR(ModifiedDate) AS year,\n",
    "        MONTH(ModifiedDate) AS month,\n",
    "        DAY(ModifiedDate) AS day\n",
    "    FROM mySQL_customer_tbl\n",
    "\"\"\")\n",
    "print(type(customer_output_df))\n",
    "customer_output_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66a3d942-acc0-4136-92f4-149d41b11d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://localhost:9000/tmp\n",
      "hdfs://localhost:9000/user\n"
     ]
    }
   ],
   "source": [
    "from py4j.java_gateway import java_import\n",
    "\n",
    "# Import necessary classes from Java\n",
    "java_import(spark._jvm, 'org.apache.hadoop.fs.FileSystem')\n",
    "java_import(spark._jvm, 'org.apache.hadoop.fs.Path')\n",
    "java_import(spark._jvm, 'org.apache.hadoop.fs.FileStatus')\n",
    "\n",
    "# Get the Hadoop configuration\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration() # current fs.defaultFS is 'file:///' which pointing to local file system.\n",
    "hadoop_conf.set(\"fs.defaultFS\", \"hdfs://localhost:9000\") # This ensures that Spark uses HDFS instead of the local file system.\n",
    "\n",
    "# Create a FileSystem object\n",
    "fs = spark._jvm.FileSystem.get(hadoop_conf)\n",
    "\n",
    "# Define the HDFS directory path\n",
    "hdfs_directory_path = spark._jvm.Path(\"/\")\n",
    "\n",
    "# List the contents of the directory\n",
    "file_statuses = fs.listStatus(hdfs_directory_path)\n",
    "\n",
    "# Iterate through the statuses and print them\n",
    "for file_status in file_statuses:\n",
    "    print(file_status.getPath())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f388b87-eafb-404f-92a1-0ffe5dd63859",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o114.listStatus.\n: java.io.FileNotFoundException: File /datalake does not exist.\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:1104)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:147)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$24.doCall(DistributedFileSystem.java:1175)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$24.doCall(DistributedFileSystem.java:1172)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:1182)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m datalake_directory_path \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/datalake\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# List the contents of the directory\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m file_statuses \u001b[38;5;241m=\u001b[39m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistStatus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatalake_directory_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Iterate through the statuses and print them\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_status \u001b[38;5;129;01min\u001b[39;00m file_statuses:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o114.listStatus.\n: java.io.FileNotFoundException: File /datalake does not exist.\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:1104)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:147)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$24.doCall(DistributedFileSystem.java:1175)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$24.doCall(DistributedFileSystem.java:1172)\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:1182)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "datalake_directory_path = spark._jvm.Path(\"/datalake\")\n",
    "\n",
    "# List the contents of the directory\n",
    "file_statuses = fs.listStatus(datalake_directory_path)\n",
    "\n",
    "# Iterate through the statuses and print them\n",
    "for file_status in file_statuses:\n",
    "    print(file_status.getPath())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b448016-4a15-452c-bebe-504d36377360",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_directory_path = spark._jvm.Path(\"/datalake/Customer\")\n",
    "# Check if the directory exists\n",
    "exists = fs.exists(customer_directory_path)\n",
    "exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeab639-3c93-405d-8969-96a6959ab720",
   "metadata": {},
   "outputs": [],
   "source": [
    "if exists:\n",
    "    # Convert Path to string\n",
    "    customer_directory_str = str(customer_directory_path.toString())\n",
    "    print(customer_directory_str)\n",
    "    datalake_customer_df = spark.read.parquet(customer_directory_str)\n",
    "    datalake_customer_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdb08d0-1d71-43ea-b6fb-bd830ffcfdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "datalake_customer_df.selectExpr(\"max(CustomerID)\").collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed9bb6b-9319-48f5-98ec-4920eecdcc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(spark))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9e0c2c",
   "metadata": {},
   "source": [
    "# Spark OOM handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab9ff1f-1e54-4e6a-b90c-c2e8477f2b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "import argparse\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86e33454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logs_dir = \"logs\"\n",
    "if not os.path.exists(logs_dir):\n",
    "    os.makedirs(logs_dir)\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=os.path.join(logs_dir, \"ingestion.log\"),\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Add ANSI escape sequences for bold yellow text\n",
    "BOLD_YELLOW = \"\\033[1m\\033[93m\"\n",
    "RESET = \"\\033[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9670875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hdfs_FileSystem_obj(spark: SparkSession, hdfs_uri: str):\n",
    "    from py4j.java_gateway import java_import\n",
    "    \n",
    "    # Import necessary classes from Java\n",
    "    java_import(spark._jvm, 'org.apache.hadoop.fs.FileSystem')\n",
    "    java_import(spark._jvm, 'org.apache.hadoop.fs.Path')\n",
    "    java_import(spark._jvm, 'org.apache.hadoop.fs.FileStatus')\n",
    "    \n",
    "    # Get the Hadoop configuration\n",
    "    hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "    hadoop_conf.set(\"fs.defaultFS\", hdfs_uri)\n",
    "\n",
    "    # Create a FileSystem object\n",
    "    return spark._jvm.FileSystem.get(hadoop_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "febf02ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/05/23 04:16:33 WARN Utils: Your hostname, Phii resolves to a loopback address: 127.0.1.1; using 172.19.26.206 instead (on interface eth0)\n",
      "24/05/23 04:16:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Exception in thread \"main\" org.apache.spark.SparkException: When running with master 'yarn' either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.\n",
      "\tat org.apache.spark.deploy.SparkSubmitArguments.error(SparkSubmitArguments.scala:650)\n",
      "\tat org.apache.spark.deploy.SparkSubmitArguments.validateSubmitArguments(SparkSubmitArguments.scala:281)\n",
      "\tat org.apache.spark.deploy.SparkSubmitArguments.validateArguments(SparkSubmitArguments.scala:237)\n",
      "\tat org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:122)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2$$anon$3.<init>(SparkSubmit.scala:1103)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.parseArguments(SparkSubmit.scala:1103)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 29\u001b[0m\n\u001b[1;32m     18\u001b[0m mysql_database_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMYSQL_DATABASE\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Create Spark session\u001b[39;00m\n\u001b[1;32m     21\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.driver.cores\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.memory\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m4g\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.executor.cores\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.jars\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/phinguyen/lib/mysql-connector-j-8.0.33.jar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myarn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIngestion - from OLTP Database (MySQL) to DataLake (Hadoop HDFS)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyspark/lib/python3.10/site-packages/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyspark/lib/python3.10/site-packages/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/miniconda3/envs/pyspark/lib/python3.10/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/pyspark/lib/python3.10/site-packages/pyspark/context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m~/miniconda3/envs/pyspark/lib/python3.10/site-packages/pyspark/java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "table_name = \"SalesOrderDetail\"\n",
    "\n",
    "# Define table-specific details\n",
    "table_details = {\n",
    "    \"Customer\": {\"primary_col\": \"CustomerID\", \"date_col\": \"ModifiedDate\"},\n",
    "    \"SalesOrderHeader\": {\"primary_col\": \"SalesOrderID\", \"date_col\": \"OrderDate\"},\n",
    "    \"SalesOrderDetail\": {\"primary_col\": \"SalesOrderID\", \"date_col\": \"ModifiedDate\"}\n",
    "}\n",
    "\n",
    "primary_col = table_details[table_name][\"primary_col\"]\n",
    "date_col = table_details[table_name][\"date_col\"]\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "mysql_user = os.getenv('MYSQL_USER')\n",
    "mysql_password = os.getenv('MYSQL_PASSWORD')\n",
    "mysql_host = os.getenv('MYSQL_HOST')\n",
    "mysql_database_name = os.getenv('MYSQL_DATABASE')\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.jars\", \"/home/phinguyen/lib/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .appName(\"Ingestion - from OLTP Database (MySQL) to DataLake (Hadoop HDFS)\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd789ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fs = get_hdfs_FileSystem_obj(spark, hdfs_uri=\"hdfs://localhost:9900\")\n",
    "table_dir_str = \"/datalake/\" + table_name\n",
    "hdfs_table_dir_path = spark._jvm.Path(table_dir_str)\n",
    "\n",
    "# Check for existing data in HDFS\n",
    "if fs.exists(hdfs_table_dir_path):\n",
    "    hdfs_df = spark.read.parquet(table_dir_str)\n",
    "    hdfs_df.createOrReplaceTempView(\"hdfs_table\")\n",
    "    latest_record = spark.sql(f\"SELECT MAX({primary_col}) FROM hdfs_table\").collect()[0][0]\n",
    "else:\n",
    "    latest_record = 0\n",
    "\n",
    "# Load new data from MySQL\n",
    "mysql_df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", f\"jdbc:mysql://{mysql_host}:3306/{mysql_database_name}\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", table_name) \\\n",
    "    .option(\"user\", mysql_user) \\\n",
    "    .option(\"password\", mysql_password) \\\n",
    "    .load()\n",
    "\n",
    "mysql_df.createOrReplaceTempView(\"mysql_table\")\n",
    "output_df = spark.sql(f\"\"\"\n",
    "                        SELECT\n",
    "                            *,\n",
    "                            YEAR({date_col}) AS year,\n",
    "                            MONTH({date_col}) AS month,\n",
    "                            DAY({date_col}) AS day\n",
    "                        FROM mysql_table\n",
    "                        WHERE {primary_col} > {latest_record}\n",
    "                        \"\"\")\n",
    "\n",
    "new_records_cnt = output_df.selectExpr(f\"COUNT({primary_col})\").collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cd0af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7783525"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_records_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e94a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17397534102201462"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_rows = 7783525\n",
    "estimated_row_size_bytes = 24\n",
    "total_memory_consumption_bytes = total_rows * estimated_row_size_bytes\n",
    "total_memory_consumption_gb = total_memory_consumption_bytes / (1024**3)  # Convert bytes to GB\n",
    "total_memory_consumption_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb27da29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 04:09:50 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "24/05/23 04:09:52 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "24/05/23 04:09:55 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "24/05/23 04:09:57 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "24/05/23 04:10:00 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "24/05/23 04:10:02 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "24/05/23 04:10:06 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "24/05/23 04:10:09 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "24/05/23 04:10:12 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "24/05/23 04:10:14 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "24/05/23 04:10:17 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "24/05/23 04:10:19 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "24/05/23 04:10:22 WARN TaskMemoryManager: Failed to allocate a page (67108864 bytes), try again.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "output_df.write.partitionBy(\"year\", \"month\", \"day\").mode(\"append\").parquet(table_dir_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b42295fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "customer_df = spark.read.parquet(\"/datalake/Customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3303624-cc6a-4baa-a2d3-f2ea9939f2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df.createOrReplaceTempView(\"customer_datalake_view\")\n",
    "count_last_date = spark.sql(\"\"\"\n",
    "    SELECT COUNT(1)\n",
    "    FROM customer_datalake_view\n",
    "    WHERE ModifiedDate = '2024-02-13'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f356e48-7913-459c-9ef7-c9e5de61b45d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[count(1): bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_last_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec4919b1-ca0a-4427-a150-e19e4599cd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  200265|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_last_date.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c8a94cc-e553-4064-b0fa-1395158991f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  3|\n",
      "|  5|\n",
      "|  7|\n",
      "|  9|\n",
      "| 11|\n",
      "| 13|\n",
      "| 15|\n",
      "| 17|\n",
      "| 19|\n",
      "| 21|\n",
      "| 23|\n",
      "| 25|\n",
      "| 27|\n",
      "| 29|\n",
      "| 31|\n",
      "| 33|\n",
      "| 35|\n",
      "| 37|\n",
      "| 39|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.range(1, 100, 2)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b41a27e-3a85-44ff-9266-5343c37034b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|5id|\n",
      "+---+\n",
      "|  5|\n",
      "| 15|\n",
      "| 25|\n",
      "| 35|\n",
      "| 45|\n",
      "| 55|\n",
      "| 65|\n",
      "| 75|\n",
      "| 85|\n",
      "| 95|\n",
      "|105|\n",
      "|115|\n",
      "|125|\n",
      "|135|\n",
      "|145|\n",
      "|155|\n",
      "|165|\n",
      "|175|\n",
      "|185|\n",
      "|195|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.selectExpr(\"id * 5 as 5id\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36a93201-fd2f-4149-b9ca-c67175bd1640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(sum(id)=2500000000000)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = spark.range(2, 10000000, 2)\n",
    "df2 = spark.range(2, 10000000, 4)\n",
    "step1 = df1.repartition(5)\n",
    "step12 = df2.repartition(6)\n",
    "step2 = step1.selectExpr(\"id * 5 as id\")\n",
    "step3 = step2.join(step12, [\"id\"])\n",
    "step4 = step3.selectExpr(\"sum(id)\")\n",
    "step4.collect() # 2500000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "066dca67-b530-4852-8c04-9f29a00f9609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=true\n",
      "+- == Final Plan ==\n",
      "   *(7) HashAggregate(keys=[], functions=[sum(id#48L)])\n",
      "   +- ShuffleQueryStage 4\n",
      "      +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=342]\n",
      "         +- *(6) HashAggregate(keys=[], functions=[partial_sum(id#48L)])\n",
      "            +- *(6) Project [id#48L]\n",
      "               +- *(6) SortMergeJoin [id#48L], [id#42L], Inner\n",
      "                  :- *(4) Sort [id#48L ASC NULLS FIRST], false, 0\n",
      "                  :  +- AQEShuffleRead coalesced\n",
      "                  :     +- ShuffleQueryStage 2\n",
      "                  :        +- Exchange hashpartitioning(id#48L, 200), ENSURE_REQUIREMENTS, [plan_id=213]\n",
      "                  :           +- *(3) Project [(id#40L * 5) AS id#48L]\n",
      "                  :              +- ShuffleQueryStage 0\n",
      "                  :                 +- Exchange RoundRobinPartitioning(5), REPARTITION_BY_NUM, [plan_id=146]\n",
      "                  :                    +- *(1) Range (2, 10000000, step=2, splits=1)\n",
      "                  +- *(5) Sort [id#42L ASC NULLS FIRST], false, 0\n",
      "                     +- AQEShuffleRead coalesced\n",
      "                        +- ShuffleQueryStage 3\n",
      "                           +- Exchange hashpartitioning(id#42L, 200), ENSURE_REQUIREMENTS, [plan_id=158]\n",
      "                              +- ShuffleQueryStage 1\n",
      "                                 +- Exchange RoundRobinPartitioning(6), REPARTITION_BY_NUM, [plan_id=154]\n",
      "                                    +- *(2) Range (2, 10000000, step=4, splits=1)\n",
      "+- == Initial Plan ==\n",
      "   HashAggregate(keys=[], functions=[sum(id#48L)])\n",
      "   +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=122]\n",
      "      +- HashAggregate(keys=[], functions=[partial_sum(id#48L)])\n",
      "         +- Project [id#48L]\n",
      "            +- SortMergeJoin [id#48L], [id#42L], Inner\n",
      "               :- Sort [id#48L ASC NULLS FIRST], false, 0\n",
      "               :  +- Exchange hashpartitioning(id#48L, 200), ENSURE_REQUIREMENTS, [plan_id=114]\n",
      "               :     +- Project [(id#40L * 5) AS id#48L]\n",
      "               :        +- Exchange RoundRobinPartitioning(5), REPARTITION_BY_NUM, [plan_id=104]\n",
      "               :           +- Range (2, 10000000, step=2, splits=1)\n",
      "               +- Sort [id#42L ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(id#42L, 200), ENSURE_REQUIREMENTS, [plan_id=115]\n",
      "                     +- Exchange RoundRobinPartitioning(6), REPARTITION_BY_NUM, [plan_id=107]\n",
      "                        +- Range (2, 10000000, step=4, splits=1)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "step4.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10663f1-1f1f-4dce-b041-51f1d310d089",
   "metadata": {},
   "source": [
    "# Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5946970-340c-4e3f-b40e-74aae35e004b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/30 23:11:54 WARN Utils: Your hostname, desktop resolves to a loopback address: 127.0.1.1; using 192.168.1.2 instead (on interface enp2s0)\n",
      "24/05/30 23:11:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/30 23:11:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession with Hive support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Python Spark SQL Hive integration example\") \\\n",
    "    .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://localhost:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4b07c70-12a0-4170-9cbc-9e1c2dc0e5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.metastore.wm.default.pool.size does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.llap.task.scheduler.preempt.independent does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.llap.output.format.arrow does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.tez.llap.min.reducer.per.executor does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.arrow.root.allocator.limit does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.vectorized.use.checked.expressions does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.tez.dynamic.semijoin.reduction.for.mapjoin does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.vectorized.complex.types.enabled does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.server2.wm.worker.threads does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.repl.partitions.dump.parallelism does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.metastore.uri.selection does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.strict.checks.no.partition.filter does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.tez.dynamic.semijoin.reduction.for.dpp.factor does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.stats.filter.in.min.ratio does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.metastore.client.cache.initial.capacity does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.stats.ndv.estimate.percent does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.server2.webui.cors.allowed.methods does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.optimize.joinreducededuplication does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.metastore.client.cache.enabled does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.stats.fetch.bitvector does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.disable.unsafe.external.table.operations does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.materializedview.rewriting.incremental does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.server2.materializedviews.registry.impl does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.metastore.event.db.notification.api.auth does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.exec.orc.delta.streaming.optimizations.enabled does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.stats.ndv.algo does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.spark.job.max.tasks does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.msck.repair.batch.max.retries does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.prewarm.spark.timeout does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.optimize.update.table.properties.from.serde.list does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.llap.plugin.client.num.threads does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.test.bucketcodec.version does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.query.reexecution.enabled does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.materializedview.rewriting.time.window does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.query.reexecution.stats.cache.batch.size does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.server2.webui.cors.allowed.headers does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.join.inner.residual does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.server2.active.passive.ha.enable does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.llap.io.trace.always.dump does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.query.reexecution.stats.persist.scope does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.mm.allow.originals does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.compactor.compact.insert.only does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.txn.xlock.iow does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.spark.rsc.conf.list does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.llap.cache.defaultfs.only.native.fileid does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.spark.optimize.shuffle.serde does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.testing.remove.logs does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.distcp.privileged.doAs does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.strict.checks.orderby.no.limit does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.metastore.client.cache.expiry.time does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.llap.io.allocator.defrag.headroom does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.notification.event.consumers does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.vectorized.input.format.supports.enabled does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.metastore.client.cache.max.capacity does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.repl.dumpdir.clean.freq does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.spark.use.ts.stats.for.mapjoin does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.repl.dump.include.acid.tables does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.server2.webui.use.pam does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.query.reexecution.max.count does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.llap.io.share.object.pools does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.optimize.update.table.properties.from.serde does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.service.metrics.codahale.reporter.classes does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.tez.session.events.print.summary does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.llap.io.vrb.queue.limit.base does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.mm.avoid.s3.globstatus does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.repl.replica.functions.root.dir does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.query.results.cache.max.entry.lifetime does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.server2.limit.connections.per.user does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.server2.thrift.http.compression.enabled does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.vectorized.execution.ptf.enabled does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.optimize.shared.work.extended does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.vectorized.row.identifier.enabled does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.query.reexecution.always.collect.operator.stats does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.repl.dumpdir.ttl does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.local.time.zone does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.server2.tez.wm.am.registry.timeout does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.server2.active.passive.ha.registry.namespace does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.create.as.insert.only does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.llap.mapjoin.memory.oversubscribe.factor does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.arrow.batch.size does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.notification.sequence.lock.retry.sleep.interval does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.repl.approx.max.load.tasks does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.query.results.cache.enabled does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.legacy.schema.for.all.serdes does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.tez.dag.status.check.interval does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.druid.bitmap.type does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.spark.dynamic.partition.pruning.map.join.only does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.llap.memory.oversubscription.max.executors.per.query does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.llap.io.trace.size does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.llap.plugin.rpc.num.handlers does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.server2.wm.allow.any.pool.via.jdbc does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.vectorized.groupby.complex.types.enabled does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.avro.timestamp.skip.conversion does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.query.results.cache.nontransactional.tables.enabled does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.stats.correlated.multi.key.joins does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.metastore.db.type does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.streaming.auto.flush.check.interval.size does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.zookeeper.connection.timeout does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.query.reexecution.strategies does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.server2.limit.connections.per.user.ipaddress does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.llap.mapjoin.memory.monitor.check.interval does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.optimize.shared.work does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.stats.estimate does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.llap.io.allocator.discard.method does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.tez.cartesian-product.enabled does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.notification.sequence.lock.max.retries does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.heap.memory.monitor.usage.threshold does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.privilege.synchronizer.interval does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.vectorized.adaptor.suppress.evaluate.exceptions does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.materializedview.rebuild.incremental does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.query.results.cache.max.entry.size does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.spark.stage.max.tasks does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.testing.short.logs does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.streaming.auto.flush.enabled does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.spark.explain.user does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.describe.partitionedtable.ignore.stats does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.server2.operation.log.cleanup.delay does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.repl.dump.metadata.only does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.optimize.countdistinct does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.auto.convert.join.shuffle.max.size does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.llap.plugin.acl does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.metastore.schema.info.class does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.server2.tez.queue.access.check does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.llap.external.splits.temp.table.storage.format does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.llap.io.row.wrapper.enabled does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.constraint.notnull.enforce does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.cli.print.escape.crlf does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.trigger.validation.interval does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.server2.webui.cors.allowed.origins does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.server2.limit.connections.per.ipaddress does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.llap.external.splits.order.by.force.single.split does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.metastore.client.cache.stats.enabled does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.notification.event.poll.interval does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.transactional.concatenate.noblock does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.materializedview.rewriting.strategy does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.vectorized.if.expr.mode does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.exim.test.mode does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.query.results.cache.directory does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.query.results.cache.wait.for.pending.results does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.remove.orderby.in.subquery does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.tez.bmj.use.subcache does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.llap.io.vrb.queue.limit.min does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.server2.wm.pool.metrics does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.repl.add.raw.reserved.namespace does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.resource.use.hdfs.location does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.stats.num.nulls.estimate.percent does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.llap.io.acid does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.llap.zk.sm.session.timeout does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.vectorized.ptf.max.memory.buffering.batch.count does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.llap.task.scheduler.am.registry does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.druid.overlord.address.default does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.optimize.remove.sq_count_check does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.server2.webui.enable.cors does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.vectorized.row.serde.inputformat.excludes does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.query.reexecution.stats.cache.size does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.combine.equivalent.work.optimization does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.lock.query.string.max.length does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.llap.io.track.cache.usage does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.use.orc.codec.pool does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.query.results.cache.max.size does not exist\n",
      "24/05/30 23:12:28 WARN HiveConf: HiveConf of name hive.repl.bootstrap.dump.open.txn.timeout does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[namespace: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c481885c-6ed7-4700-97f9-51186eb96f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------------+\n",
      "|ProductCategoryKey|EnglishProductCategoryName|\n",
      "+------------------+--------------------------+\n",
      "|                 4|               Accessories|\n",
      "|                 1|                     Bikes|\n",
      "|                 3|                  Clothing|\n",
      "|                 2|                Components|\n",
      "+------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ProductCategory_df = spark.read.parquet(\"hdfs://localhost:9000/datalake/ProductCategory\")\n",
    "ProductCategory_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51a92833-526a-4f87-8513-580d307ff69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to a Hive table\n",
    "ProductCategory_df.write.mode('overwrite').saveAsTable(\"ProductCategory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dd12034-ed25-4783-9c80-26dc94e21e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-----------+\n",
      "|namespace|      tableName|isTemporary|\n",
      "+---------+---------------+-----------+\n",
      "|  default|productcategory|      false|\n",
      "+---------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57b4850b-e487-4797-8db4-347b73853920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------------------+--------------------+------+------------+---------+\n",
      "|ProductKey|ProductAlternateKey|ProductCategoryKey|  EnglishProductName| Color|StandardCost|ListPrice|\n",
      "+----------+-------------------+------------------+--------------------+------+------------+---------+\n",
      "|         1|            AR-5381|              NULL|     Adjustable Race|  NULL|        NULL|     NULL|\n",
      "|         2|            BA-8327|              NULL|        Bearing Ball|  NULL|        NULL|     NULL|\n",
      "|         3|            BE-2349|              NULL|     BB Ball Bearing|  NULL|        NULL|     NULL|\n",
      "|         4|            BE-2908|              NULL|Headset Ball Bear...|  NULL|        NULL|     NULL|\n",
      "|         5|            BL-2036|              NULL|               Blade|  NULL|        NULL|     NULL|\n",
      "|         6|            CA-5965|              NULL|         LL Crankarm| Black|        NULL|     NULL|\n",
      "|         7|            CA-6738|              NULL|         ML Crankarm| Black|        NULL|     NULL|\n",
      "|         8|            CA-7457|              NULL|         HL Crankarm| Black|        NULL|     NULL|\n",
      "|         9|            CB-2903|              NULL|     Chainring Bolts|Silver|        NULL|     NULL|\n",
      "|        10|            CN-6137|              NULL|       Chainring Nut|Silver|        NULL|     NULL|\n",
      "|        11|            CR-7833|              NULL|           Chainring| Black|        NULL|     NULL|\n",
      "|        12|            CR-9981|              NULL|          Crown Race|  NULL|        NULL|     NULL|\n",
      "|        13|            CS-2812|              NULL|         Chain Stays|  NULL|        NULL|     NULL|\n",
      "|        14|            DC-8732|              NULL|             Decal 1|  NULL|        NULL|     NULL|\n",
      "|        15|            DC-9824|              NULL|             Decal 2|  NULL|        NULL|     NULL|\n",
      "|        16|            DT-2377|              NULL|           Down Tube|  NULL|        NULL|     NULL|\n",
      "|        17|            EC-M092|              NULL|   Mountain End Caps|  NULL|        NULL|     NULL|\n",
      "|        18|            EC-R098|              NULL|       Road End Caps|  NULL|        NULL|     NULL|\n",
      "|        19|            EC-T209|              NULL|    Touring End Caps|  NULL|        NULL|     NULL|\n",
      "|        20|            FE-3760|              NULL|            Fork End|  NULL|        NULL|     NULL|\n",
      "+----------+-------------------+------------------+--------------------+------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Product_df = spark.read.parquet(\"hdfs://localhost:9000/datalake/Product\")\n",
    "Product_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b56c9c7-b562-4818-9fc6-d65e2fa0c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to a Hive table\n",
    "Product_df.write.mode('overwrite').saveAsTable(\"Product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db42917b-90da-44de-885e-01e9c54d4c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+--------------------+---+-------------------+----+-----+---+\n",
      "|CustomerID|            Name|               Email|Age|       ModifiedDate|year|month|day|\n",
      "+----------+----------------+--------------------+---+-------------------+----+-----+---+\n",
      "|   1700291|   Yvonne Zamora|  odavis@example.net| 19|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|   1700292|    Brett Little|sandra01@example.com| 33|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|   1700293|   Anthony Hayes|alvarezwesley@exa...| 30|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|   1700294|  Daniel Sanchez|sgarrison@example...| 73|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|   1700295|   Abigail Miles|    dcox@example.org| 80|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|   1700296|    Jerome Kelly| kelly26@example.com| 89|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|   1700297|     Frank Weber|rangelrichard@exa...| 84|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|   1700298| Amanda Martinez|   dcook@example.org| 63|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|   1700299|  Bryan Phillips|pearsonteresa@exa...| 85|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|   1700300|Spencer Gonzales|jenniferjimenez@e...| 85|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|   1700301|   Stacie Fields| mharvey@example.org| 47|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|   1700302| Robert Williams|philippayne@examp...| 48|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|   1700303|   Denise Hughes|christian86@examp...| 45|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|   1700304|    Rachel Adams|  john28@example.org| 43|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|   1700305| Gregory Johnson|gonzalesrobert@ex...| 21|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|   1700306|    Carol Willis|pettyhector@examp...| 76|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|   1700307| Timothy Herrera|ymccullough@examp...| 57|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|   1700308|      Mike Adams|  ujones@example.org| 61|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|   1700309|      Erin Brown|  sean06@example.org| 20|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|   1700310| Jessica Morales|michael63@example...| 21|2024-02-03 00:00:00|2024|    2|  3|\n",
      "+----------+----------------+--------------------+---+-------------------+----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Customer_df = spark.read.parquet(\"hdfs://localhost:9000/datalake/Customer\")\n",
    "Customer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83328039-d9e3-431c-9e59-be9f24bf4f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Customer_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a613d7ee-6f52-43df-afe5-847b2931172a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/31 01:12:57 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/05/31 01:12:57 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/05/31 01:12:57 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "24/05/31 01:12:57 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/05/31 01:12:57 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/05/31 01:12:57 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/05/31 01:12:57 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/05/31 01:12:57 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/05/31 01:12:57 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/05/31 01:12:57 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/05/31 01:12:57 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/05/31 01:12:57 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/05/31 01:12:57 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "24/05/31 01:12:57 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 61.43% for 11 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 61.43% for 11 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 61.43% for 11 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 61.43% for 11 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 61.43% for 11 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 61.43% for 11 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 61.43% for 11 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 61.43% for 11 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 61.43% for 11 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/05/31 01:12:58 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "Customer_df.write.mode(\"overwrite\").partitionBy(\"year\", \"month\", \"day\").saveAsTable(\"Customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2255263-c5a8-4803-bee1-fa213d8fe015",
   "metadata": {},
   "outputs": [],
   "source": [
    "SalesOrderHeader_df = spark.read.parquet(\"hdfs://localhost:9000/datalake/SalesOrderHeader\")\n",
    "SalesOrderDetail_df = spark.read.parquet(\"hdfs://localhost:9000/datalake/SalesOrderDetail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91f322e5-d48d-4149-99e8-c8e82478341c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+----------+----+-----+---+\n",
      "|SalesOrderID|          OrderDate|CustomerID|year|month|day|\n",
      "+------------+-------------------+----------+----+-----+---+\n",
      "|    12749999|2024-02-03 00:00:00|   1221573|2024|    2|  3|\n",
      "|    12750000|2024-02-03 00:00:00|   2500598|2024|    2|  3|\n",
      "|    12750001|2024-02-03 00:00:00|    285605|2024|    2|  3|\n",
      "|    12750002|2024-02-03 00:00:00|   2550865|2024|    2|  3|\n",
      "|    12750003|2024-02-03 00:00:00|    469799|2024|    2|  3|\n",
      "|    12750004|2024-02-03 00:00:00|    865548|2024|    2|  3|\n",
      "|    12750005|2024-02-03 00:00:00|    177726|2024|    2|  3|\n",
      "|    12750006|2024-02-03 00:00:00|    804324|2024|    2|  3|\n",
      "|    12750007|2024-02-03 00:00:00|     65563|2024|    2|  3|\n",
      "|    12750008|2024-02-03 00:00:00|    325184|2024|    2|  3|\n",
      "|    12750009|2024-02-03 00:00:00|    427994|2024|    2|  3|\n",
      "|    12750010|2024-02-03 00:00:00|   1325818|2024|    2|  3|\n",
      "|    12750011|2024-02-03 00:00:00|   2156655|2024|    2|  3|\n",
      "|    12750012|2024-02-03 00:00:00|    438379|2024|    2|  3|\n",
      "|    12750013|2024-02-03 00:00:00|   1777251|2024|    2|  3|\n",
      "|    12750014|2024-02-03 00:00:00|    580542|2024|    2|  3|\n",
      "|    12750015|2024-02-03 00:00:00|   2195228|2024|    2|  3|\n",
      "|    12750016|2024-02-03 00:00:00|    402833|2024|    2|  3|\n",
      "|    12750017|2024-02-03 00:00:00|   1207567|2024|    2|  3|\n",
      "|    12750018|2024-02-03 00:00:00|   2144713|2024|    2|  3|\n",
      "+------------+-------------------+----------+----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SalesOrderHeader_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "929ed3fa-cd1a-4b7d-9530-8415d522d3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+----------+---+-------------------+----+-----+---+\n",
      "|SalesOrderID|SalesOrderLineNumber|ProductKey|Qty|       ModifiedDate|year|month|day|\n",
      "+------------+--------------------+----------+---+-------------------+----+-----+---+\n",
      "|    13033328|                   1|       130|  9|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|    13033328|                   2|       499|  5|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|    13033328|                   3|       366| 16|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|    13033328|                   4|       306|  1|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|    13033329|                   1|       586|  4|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|    13033329|                   2|        54|  2|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|    13033329|                   3|        77|  8|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|    13033329|                   4|        12| 10|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|    13033329|                   5|        81| 15|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|    13033329|                   6|       527| 15|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|    13033329|                   7|       551| 16|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|    13033329|                   8|       519|  3|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|    13033330|                   1|       310| 17|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|    13033330|                   2|       551| 19|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|    13033330|                   3|       144| 13|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|    13033330|                   4|        16|  8|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|    13033331|                   1|       593|  5|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|    13033331|                   2|       466|  2|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|    13033331|                   3|       428| 12|2024-02-03 00:00:00|2024|    2|  3|\n",
      "|    13033331|                   4|       585| 11|2024-02-03 00:00:00|2024|    2|  3|\n",
      "+------------+--------------------+----------+---+-------------------+----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SalesOrderDetail_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33f654ff-dd39-4d1e-9759-a96e1dfdde9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+----------+----+-----+---+\n",
      "|SalesOrderID|          OrderDate|CustomerID|year|month|day|\n",
      "+------------+-------------------+----------+----+-----+---+\n",
      "|    12749999|2024-02-03 00:00:00|   1221573|2024|    2|  3|\n",
      "|    12750000|2024-02-03 00:00:00|   2500598|2024|    2|  3|\n",
      "|    12750001|2024-02-03 00:00:00|    285605|2024|    2|  3|\n",
      "|    12750002|2024-02-03 00:00:00|   2550865|2024|    2|  3|\n",
      "|    12750003|2024-02-03 00:00:00|    469799|2024|    2|  3|\n",
      "|    12750004|2024-02-03 00:00:00|    865548|2024|    2|  3|\n",
      "|    12750005|2024-02-03 00:00:00|    177726|2024|    2|  3|\n",
      "|    12750006|2024-02-03 00:00:00|    804324|2024|    2|  3|\n",
      "|    12750007|2024-02-03 00:00:00|     65563|2024|    2|  3|\n",
      "|    12750008|2024-02-03 00:00:00|    325184|2024|    2|  3|\n",
      "|    12750009|2024-02-03 00:00:00|    427994|2024|    2|  3|\n",
      "|    12750010|2024-02-03 00:00:00|   1325818|2024|    2|  3|\n",
      "|    12750011|2024-02-03 00:00:00|   2156655|2024|    2|  3|\n",
      "|    12750012|2024-02-03 00:00:00|    438379|2024|    2|  3|\n",
      "|    12750013|2024-02-03 00:00:00|   1777251|2024|    2|  3|\n",
      "|    12750014|2024-02-03 00:00:00|    580542|2024|    2|  3|\n",
      "|    12750015|2024-02-03 00:00:00|   2195228|2024|    2|  3|\n",
      "|    12750016|2024-02-03 00:00:00|    402833|2024|    2|  3|\n",
      "|    12750017|2024-02-03 00:00:00|   1207567|2024|    2|  3|\n",
      "|    12750018|2024-02-03 00:00:00|   2144713|2024|    2|  3|\n",
      "+------------+-------------------+----------+----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SalesOrderHeader_df.join(Customer_df, SalesOrderHeader_df.CustomerID == Customer_df.CustomerID)\n",
    "SalesOrderHeader_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d592b94e-9cbc-46e6-95bf-5d1032bf2cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:=============>  (10 + 2) / 12][Stage 26:=======>         (5 + 6) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+----------+----+-----+---+----------+------------+--------------------+---+-------------------+----+-----+---+\n",
      "|SalesOrderID|          OrderDate|CustomerID|year|month|day|CustomerID|        Name|               Email|Age|       ModifiedDate|year|month|day|\n",
      "+------------+-------------------+----------+----+-----+---+----------+------------+--------------------+---+-------------------+----+-----+---+\n",
      "|    14137365|2024-02-03 00:00:00|        28|2024|    2|  3|        28|Tyler Hester|njohnson@example.org| 55|2024-01-01 00:00:00|2024|    1|  1|\n",
      "|     6990868|2024-02-03 00:00:00|        28|2024|    2|  3|        28|Tyler Hester|njohnson@example.org| 55|2024-01-01 00:00:00|2024|    1|  1|\n",
      "|     8555951|2024-02-03 00:00:00|        28|2024|    2|  3|        28|Tyler Hester|njohnson@example.org| 55|2024-01-01 00:00:00|2024|    1|  1|\n",
      "|     9445600|2024-02-03 00:00:00|        28|2024|    2|  3|        28|Tyler Hester|njohnson@example.org| 55|2024-01-01 00:00:00|2024|    1|  1|\n",
      "|     3732533|2024-02-01 00:00:00|        28|2024|    2|  1|        28|Tyler Hester|njohnson@example.org| 55|2024-01-01 00:00:00|2024|    1|  1|\n",
      "|     4465556|2024-02-02 00:00:00|        28|2024|    2|  2|        28|Tyler Hester|njohnson@example.org| 55|2024-01-01 00:00:00|2024|    1|  1|\n",
      "|     1246743|2024-01-13 00:00:00|        28|2024|    1| 13|        28|Tyler Hester|njohnson@example.org| 55|2024-01-01 00:00:00|2024|    1|  1|\n",
      "|      944747|2024-01-10 00:00:00|        28|2024|    1| 10|        28|Tyler Hester|njohnson@example.org| 55|2024-01-01 00:00:00|2024|    1|  1|\n",
      "|      100615|2024-01-02 00:00:00|        28|2024|    1|  2|        28|Tyler Hester|njohnson@example.org| 55|2024-01-01 00:00:00|2024|    1|  1|\n",
      "|      824193|2024-01-09 00:00:00|        28|2024|    1|  9|        28|Tyler Hester|njohnson@example.org| 55|2024-01-01 00:00:00|2024|    1|  1|\n",
      "|      864341|2024-01-09 00:00:00|        28|2024|    1|  9|        28|Tyler Hester|njohnson@example.org| 55|2024-01-01 00:00:00|2024|    1|  1|\n",
      "|      867535|2024-01-09 00:00:00|        28|2024|    1|  9|        28|Tyler Hester|njohnson@example.org| 55|2024-01-01 00:00:00|2024|    1|  1|\n",
      "|      577220|2024-01-06 00:00:00|        28|2024|    1|  6|        28|Tyler Hester|njohnson@example.org| 55|2024-01-01 00:00:00|2024|    1|  1|\n",
      "|      214435|2024-01-03 00:00:00|        28|2024|    1|  3|        28|Tyler Hester|njohnson@example.org| 55|2024-01-01 00:00:00|2024|    1|  1|\n",
      "|      224438|2024-01-03 00:00:00|        28|2024|    1|  3|        28|Tyler Hester|njohnson@example.org| 55|2024-01-01 00:00:00|2024|    1|  1|\n",
      "|      262467|2024-01-03 00:00:00|        28|2024|    1|  3|        28|Tyler Hester|njohnson@example.org| 55|2024-01-01 00:00:00|2024|    1|  1|\n",
      "|      298525|2024-01-03 00:00:00|        28|2024|    1|  3|        28|Tyler Hester|njohnson@example.org| 55|2024-01-01 00:00:00|2024|    1|  1|\n",
      "|     1965139|2024-01-20 00:00:00|        28|2024|    1| 20|        28|Tyler Hester|njohnson@example.org| 55|2024-01-01 00:00:00|2024|    1|  1|\n",
      "|     1603148|2024-01-17 00:00:00|        28|2024|    1| 17|        28|Tyler Hester|njohnson@example.org| 55|2024-01-01 00:00:00|2024|    1|  1|\n",
      "|     1625058|2024-01-17 00:00:00|        28|2024|    1| 17|        28|Tyler Hester|njohnson@example.org| 55|2024-01-01 00:00:00|2024|    1|  1|\n",
      "+------------+-------------------+----------+----+-----+---+----------+------------+--------------------+---+-------------------+----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "join_df = SalesOrderHeader_df.join(Customer_df, SalesOrderHeader_df.CustomerID == Customer_df.CustomerID)\n",
    "join_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c0f455a-6539-4a9a-b7be-46688d949279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|SalesOrderID|              Name|\n",
      "+------------+------------------+\n",
      "|    17000000|   Michele Stevens|\n",
      "|    16999999|     Carla Ramirez|\n",
      "|    16999998|     Brandon Banks|\n",
      "|    16999997|         Monica Ho|\n",
      "|    16999996|      David Austin|\n",
      "|    16999995|Mrs. Susan Mathews|\n",
      "|    16999994|       Rachel Kirk|\n",
      "|    16999993|   Kristina Tucker|\n",
      "|    16999992| Mr. Roberto Walls|\n",
      "|    16999991|     Cody Cummings|\n",
      "|    16999990|  Michelle Jackson|\n",
      "|    16999989|   Kristine Vaughn|\n",
      "|    16999988|         Brian Day|\n",
      "|    16999987|      Megan Kramer|\n",
      "|    16999986|   Patricia Ibarra|\n",
      "|    16999985|    Heidi Thompson|\n",
      "|    16999984|      Jeremy Gomez|\n",
      "|    16999983|      Shawn Thomas|\n",
      "|    16999982|       Audrey Bray|\n",
      "|    16999981|    Kimberly Meyer|\n",
      "+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "\n",
    "join_df.selectExpr(\"SalesOrderID\", \"Name\").sort(desc(\"SalesOrderID\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24d1aef-1bd2-4444-a70a-339f97906df5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
