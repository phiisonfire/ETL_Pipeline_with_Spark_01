## bash usages

1. create database schema for mysql

python3 scripts/02_create_retail_db_schema.py

2. load table Product, ProductCategory from .csv files into corresponding tables in mysql

python3 scripts/01_load_database_table_from_csv.py \
--csv_table_file_path /app/sample_data/ProductCategory.csv \
--csv_table_schema_file_path /app/sample_data/ProductCategory.yaml \
--table_name ProductCategory

python3 scripts/01_load_database_table_from_csv.py \
--csv_table_file_path /app/sample_data/Product.csv \
--csv_table_schema_file_path /app/sample_data/Product.yaml \
--table_name Product

3. generating new (fake) data
python3 scripts/03_generate_multiple_date_sales_data_with_load.py \
-n 10000 \
-sdate 2024-02-15 \
-edate 2024-03-15

4. ingest fixed tables (ProductCategory & Product), without partitioning
/opt/spark/bin/spark-submit \
--master yarn \
--deploy-mode client \
--driver-cores 1 \
--driver-memory 1g \
--executor-cores 2 \
--executor-memory 2g \
/app/src/pipelines/ingestion_fixed_table.py \
--table_name ProductCategory

5. ingest Customers, SalesOrderHeader and SalesOrderDetail tables
/opt/spark/bin/spark-submit \
--master yarn \
--deploy-mode client \
--driver-cores 1 \
--driver-memory 1g \
--executor-cores 2 \
--executor-memory 2g \
/app/src/pipelines/ingestion.py \
--table_name Customer

6. ETL datalake -> Hive

/opt/spark/bin/spark-submit \
--master yarn \
--deploy-mode client \
--driver-cores 1 \
--driver-memory 1g \
--executor-cores 2 \
--executor-memory 2g \
/app/src/pipelines/transformation.py
